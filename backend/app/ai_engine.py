"""
ü§ñ KI-QMS Advanced AI Engine v2.0
Erweiterte KI-Funktionalit√§ten f√ºr intelligente Dokumentenanalyse

Features:
- üåç Automatische Spracherkennung (DE/EN/FR)
- üìä Verbesserte Dokumenttyp-Klassifikation (95%+ Genauigkeit)
- üìã Intelligente Norm-Referenz-Extraktion
- ‚öñÔ∏è Compliance-Gap-Analyse
- üîç √Ñhnlichkeits-basierte Duplikatserkennung
- üè∑Ô∏è Erweiterte Metadaten-Extraktion
- üÜì Kostenlose KI-Provider (Ollama, Hugging Face)
"""

import re
import string
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from enum import Enum
import logging
from pathlib import Path

# Kostenlose KI-Provider importieren
try:
    from .ai_providers import OllamaProvider, HuggingFaceProvider, GoogleGeminiProvider
except ImportError:
    # Fallback wenn Provider nicht verf√ºgbar
    OllamaProvider = None
    HuggingFaceProvider = None
    GoogleGeminiProvider = None

logger = logging.getLogger(__name__)

class DocumentLanguage(str, Enum):
    """Unterst√ºtzte Dokumentsprachen"""
    GERMAN = "de"
    ENGLISH = "en" 
    FRENCH = "fr"
    MIXED = "mixed"
    UNKNOWN = "unknown"

class ConfidenceLevel(str, Enum):
    """Konfidenz-Level f√ºr KI-Erkennungen"""
    VERY_HIGH = "very_high"  # 95-100%
    HIGH = "high"            # 85-94%
    MEDIUM = "medium"        # 70-84%
    LOW = "low"             # 50-69%
    VERY_LOW = "very_low"   # <50%

@dataclass
class AIAnalysisResult:
    """Ergebnis einer umfassenden KI-Analyse"""
    # Spracherkennung
    detected_language: DocumentLanguage
    language_confidence: float
    language_details: Dict[str, float]
    
    # Dokumentklassifikation
    document_type: str
    type_confidence: float
    type_alternatives: List[Tuple[str, float]]
    
    # Norm-Referenzen
    norm_references: List[Dict[str, Union[str, float]]]
    compliance_keywords: List[str]
    
    # Metadaten
    extracted_keywords: List[str]
    complexity_score: int
    risk_level: str
    
    # Qualit√§tsbewertung
    content_quality_score: float
    completeness_score: float
    
    # √Ñhnlichkeitsanalyse
    potential_duplicates: List[Dict[str, Union[int, str, float]]]

class AdvancedAIEngine:
    """Erweiterte KI-Engine f√ºr QMS-Dokumentenanalyse"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self._init_language_patterns()
        self._init_document_type_patterns()
        self._init_norm_patterns()
        self._init_compliance_keywords()
        self._init_ai_providers()
        
    def _init_ai_providers(self):
        """Initialisiert kostenlose KI-Provider"""
        self.ai_providers = {}
        
        # Ollama Provider (lokal, kostenlos)
        if OllamaProvider:
            try:
                self.ai_providers['ollama'] = OllamaProvider()
                self.logger.info("ü§ñ Ollama Provider initialisiert")
            except Exception as e:
                self.logger.warning(f"Ollama Provider nicht verf√ºgbar: {e}")
        
        # Google Gemini Provider (kostenlos, 1500 Anfragen/Tag)
        if GoogleGeminiProvider:
            try:
                self.ai_providers['google_gemini'] = GoogleGeminiProvider()
                self.logger.info("üåü Google Gemini Provider initialisiert")
            except Exception as e:
                self.logger.warning(f"Google Gemini Provider nicht verf√ºgbar: {e}")
        
        # Hugging Face Provider (kostenlos mit Limits)
        if HuggingFaceProvider:
            try:
                self.ai_providers['huggingface'] = HuggingFaceProvider()
                self.logger.info("ü§ó Hugging Face Provider initialisiert")
            except Exception as e:
                self.logger.warning(f"Hugging Face Provider nicht verf√ºgbar: {e}")
    
    async def ai_enhanced_analysis(self, text: str, document_type: str = "unknown") -> Dict[str, Any]:
        """
        üß† KI-unterst√ºtzte Dokumentanalyse mit kostenlosen Providern
        
        Args:
            text: Dokumenttext
            document_type: Dokumenttyp
            
        Returns:
            Dict: KI-Analyseergebnisse
        """
        self.logger.info("ü§ñ Starte KI-unterst√ºtzte Analyse...")
        
        # Provider-Priorit√§t: Ollama ‚Üí Google Gemini ‚Üí Hugging Face ‚Üí Rule-based
        
        # 1. Versuche Ollama zuerst (lokal, schnell, offline)
        if 'ollama' in self.ai_providers:
            try:
                ollama_available = await self.ai_providers['ollama'].is_available()
                if ollama_available:
                    self.logger.info("üöÄ Nutze Ollama f√ºr lokale KI-Analyse")
                    result = await self.ai_providers['ollama'].analyze_document(text, document_type)
                    result['provider'] = 'ollama'
                    result['cost'] = 'kostenlos'
                    return result
            except Exception as e:
                self.logger.warning(f"Ollama Analyse fehlgeschlagen: {e}")
        
        # 2. Fallback: Google Gemini (kostenlos, 1500/Tag)
        if 'google_gemini' in self.ai_providers:
            try:
                gemini_available = await self.ai_providers['google_gemini'].is_available()
                if gemini_available:
                    self.logger.info("üåü Nutze Google Gemini f√ºr KI-Analyse")
                    result = await self.ai_providers['google_gemini'].analyze_document(text, document_type)
                    result['provider'] = 'google_gemini'
                    result['cost'] = 'kostenlos (1500/Tag)'
                    return result
            except Exception as e:
                self.logger.warning(f"Google Gemini Analyse fehlgeschlagen: {e}")
        
        # 3. Fallback: Hugging Face
        if 'huggingface' in self.ai_providers:
            try:
                self.logger.info("ü§ó Nutze Hugging Face als Fallback")
                result = await self.ai_providers['huggingface'].analyze_document(text, document_type)
                result['provider'] = 'huggingface'
                result['cost'] = 'kostenlos (limitiert)'
                return result
            except Exception as e:
                self.logger.warning(f"Hugging Face Analyse fehlgeschlagen: {e}")
        
        # Letzter Fallback: Regel-basierte Analyse
        self.logger.info("üìã Nutze regel-basierte Fallback-Analyse")
        return {
            'document_type': document_type,
            'main_topics': self._extract_keywords(text)[:5],
            'language': str(self.detect_language(text)[0]),
            'quality_score': self.assess_content_quality(text)[0] * 10,
            'compliance_relevant': any(kw in text.lower() for kw in ['iso', 'norm', 'standard', 'compliance']),
            'ai_summary': f"Regel-basierte Analyse: {len(text)} Zeichen, {text.count('.') + text.count('!') + text.count('?')} S√§tze",
            'provider': 'regel-basiert',
            'cost': 'kostenlos'
        }
        
    def _init_language_patterns(self):
        """Initialisiert Spracherkennungs-Patterns"""
        self.language_patterns = {
            DocumentLanguage.GERMAN: {
                'common_words': [
                    'und', 'der', 'die', 'das', 'ist', 'zu', 'eine', 'von', 'mit', 'auf',
                    'f√ºr', 'als', 'werden', 'wird', 'durch', 'nach', 'bei', 'um', '√ºber',
                    'dokument', 'verfahren', 'prozess', 'qualit√§t', 'management', 'system',
                    'anforderung', 'pr√ºfung', 'kontrolle', '√ºberwachung', 'bewertung'
                ],
                'medical_terms': [
                    'medizinprodukt', 'risikomanagement', 'klinische', 'bewertung',
                    'konformit√§tsbewertung', 'ce-kennzeichnung', 'technische', 'dokumentation',
                    'gebrauchsanweisung', 'anwender', 'patient', 'sicherheit'
                ],
                'qms_terms': [
                    'qualit√§tsmanagementsystem', 'qms', 'qualit√§tssicherung',
                    'qualit√§tskontrolle', 'validierung', 'verifizierung', 'kalibrierung'
                ],
                'stopwords': {'der', 'die', 'das', 'und', 'oder', 'aber', 'wenn', 'dann'}
            },
            DocumentLanguage.ENGLISH: {
                'common_words': [
                    'and', 'the', 'of', 'to', 'in', 'for', 'is', 'on', 'that', 'by',
                    'this', 'with', 'from', 'they', 'we', 'been', 'have', 'had', 'their',
                    'document', 'procedure', 'process', 'quality', 'management', 'system',
                    'requirement', 'testing', 'control', 'monitoring', 'assessment'
                ],
                'medical_terms': [
                    'medical', 'device', 'risk', 'management', 'clinical', 'evaluation',
                    'conformity', 'assessment', 'ce', 'marking', 'technical', 'documentation',
                    'user', 'instructions', 'patient', 'safety', 'regulatory'
                ],
                'qms_terms': [
                    'quality', 'management', 'system', 'qms', 'assurance',
                    'control', 'validation', 'verification', 'calibration'
                ],
                'stopwords': {'the', 'and', 'or', 'but', 'if', 'then', 'this', 'that'}
            },
            DocumentLanguage.FRENCH: {
                'common_words': [
                    'et', 'de', 'le', 'la', 'les', 'des', 'du', 'une', 'un', 'pour',
                    'dans', 'sur', 'avec', 'par', '√™tre', 'avoir', 'que', 'qui', 'ce',
                    'document', 'proc√©dure', 'processus', 'qualit√©', 'gestion', 'syst√®me'
                ],
                'medical_terms': [
                    'dispositif', 'm√©dical', 'gestion', 'risques', '√©valuation',
                    'clinique', 'conformit√©', 'marquage', 'ce', 'documentation',
                    'technique', 'notice', 'utilisation', 'patient', 's√©curit√©'
                ],
                'qms_terms': [
                    'syst√®me', 'management', 'qualit√©', 'smq', 'assurance',
                    'contr√¥le', 'validation', 'v√©rification', '√©talonnage'
                ],
                'stopwords': {'le', 'la', 'les', 'et', 'ou', 'mais', 'si', 'alors'}
            }
        }
    
    def _init_document_type_patterns(self):
        """Initialisiert erweiterte Dokumenttyp-Erkennungspatterns"""
        self.document_type_patterns = {
            'QM_MANUAL': {
                'keywords': [
                    'qualit√§tsmanagement', 'quality management', 'qm-handbuch', 'qm manual',
                    'qualit√§tsmanagementsystem', 'quality management system', 'qms',
                    'unternehmenshandbuch', 'company manual', 'organisationshandbuch'
                ],
                'indicators': [
                    'iso 13485', 'iso 9001', 'qualit√§tspolitik', 'quality policy',
                    'verantwortung der leitung', 'management responsibility',
                    'qualit√§tsziele', 'quality objectives'
                ],
                'structure_hints': ['kapitel', 'chapter', 'abschnitt', 'section', 'anhang', 'appendix']
            },
            'SOP': {
                'keywords': [
                    'standard operating procedure', 'sop', 'arbeitsanweisung',
                    'verfahrensanweisung', 'betriebsanweisung', 'work instruction',
                    'standardverfahren', 'standard procedure'
                ],
                'indicators': [
                    'schritt', 'step', 'durchf√ºhrung', 'execution', 'anleitung',
                    'instruction', 'vorgehensweise', 'procedure', 'ablauf', 'workflow'
                ],
                'structure_hints': ['schritt f√ºr schritt', 'step by step', 'checkliste', 'checklist']
            },
            'RISK_ASSESSMENT': {
                'keywords': [
                    'risikoanalyse', 'risk analysis', 'risikobewertung', 'risk assessment',
                    'risikomanagement', 'risk management', 'risikobeurteilung',
                    'hazard analysis', 'gef√§hrdungsanalyse'
                ],
                'indicators': [
                    'iso 14971', 'risiko', 'risk', 'gef√§hrdung', 'hazard',
                    'wahrscheinlichkeit', 'probability', 'schweregrad', 'severity',
                    'risikomatrix', 'risk matrix', 'risikoakzeptanz', 'risk acceptance'
                ],
                'structure_hints': ['risikobewertung', 'risk evaluation', 'ma√ünahmen', 'measures']
            },
            'VALIDATION_PROTOCOL': {
                'keywords': [
                    'validierung', 'validation', 'validierungsprotokoll', 'validation protocol',
                    'qualifizierung', 'qualification', 'verifikation', 'verification',
                    'pr√ºfprotokoll', 'test protocol'
                ],
                'indicators': [
                    'testplan', 'test plan', 'pr√ºfkriterien', 'acceptance criteria',
                    'ergebnis', 'results', 'bewertung', 'evaluation',
                    'freigabe', 'release', 'genehmigung', 'approval'
                ],
                'structure_hints': ['test', 'pr√ºfung', 'protokoll', 'protocol', 'ergebnis', 'result']
            },
            'CALIBRATION_PROCEDURE': {
                'keywords': [
                    'kalibrierung', 'calibration', 'kalibrierprozedur', 'calibration procedure',
                    'eichung', 'adjustment', 'justierung', 'messtechnik', 'metrology'
                ],
                'indicators': [
                    'messger√§t', 'measuring equipment', 'normal', 'standard',
                    'messunsicherheit', 'measurement uncertainty', 'r√ºckf√ºhrbarkeit',
                    'traceability', 'kalibrierintervall', 'calibration interval'
                ],
                'structure_hints': ['messverfahren', 'measurement procedure', 'kalibrierung', 'calibration']
            },
            'AUDIT_REPORT': {
                'keywords': [
                    'audit', 'auditbericht', 'audit report', 'begutachtung',
                    '√ºberpr√ºfung', 'review', 'inspektion', 'inspection'
                ],
                'indicators': [
                    'feststellung', 'finding', 'abweichung', 'deviation',
                    'nichtkonformit√§t', 'non-conformity', 'korrekturma√ünahme',
                    'corrective action', 'empfehlung', 'recommendation'
                ],
                'structure_hints': ['auditfeststellung', 'audit finding', 'bewertung', 'assessment']
            }
        }
    
    def _init_norm_patterns(self):
        """Initialisiert Norm-Erkennungspatterns"""
        self.norm_patterns = {
            # ISO Normen
            r'ISO\s*13485(?::?\s*(\d{4}))?': {
                'name': 'ISO 13485',
                'type': 'medical_devices_qms',
                'description': 'Medical devices - Quality management systems'
            },
            r'ISO\s*14971(?::?\s*(\d{4}))?': {
                'name': 'ISO 14971', 
                'type': 'risk_management',
                'description': 'Medical devices - Application of risk management'
            },
            r'ISO\s*9001(?::?\s*(\d{4}))?': {
                'name': 'ISO 9001',
                'type': 'quality_management',
                'description': 'Quality management systems - Requirements'
            },
            r'ISO\s*27001(?::?\s*(\d{4}))?': {
                'name': 'ISO 27001',
                'type': 'information_security',
                'description': 'Information security management systems'
            },
            
            # EU Regulierungen
            r'(?:EU\s*)?MDR\s*(?:2017/745|745/2017)': {
                'name': 'EU MDR 2017/745',
                'type': 'medical_device_regulation',
                'description': 'European Medical Device Regulation'
            },
            r'(?:EU\s*)?IVDR\s*(?:2017/746|746/2017)': {
                'name': 'EU IVDR 2017/746',
                'type': 'ivd_regulation', 
                'description': 'In Vitro Diagnostic Medical Devices Regulation'
            },
            
            # FDA Regulierungen
            r'(?:FDA\s*)?21\s*CFR\s*(?:Part\s*)?820': {
                'name': 'FDA 21 CFR Part 820',
                'type': 'quality_system_regulation',
                'description': 'Quality System Regulation'
            },
            r'(?:FDA\s*)?21\s*CFR\s*(?:Part\s*)?11': {
                'name': 'FDA 21 CFR Part 11',
                'type': 'electronic_records',
                'description': 'Electronic Records; Electronic Signatures'
            },
            
            # IEC Normen
            r'IEC\s*62304(?::?\s*(\d{4}))?': {
                'name': 'IEC 62304',
                'type': 'medical_software',
                'description': 'Medical device software - Software life cycle processes'
            },
            r'IEC\s*60601(?:-\d+)?(?:-\d+)?(?::?\s*(\d{4}))?': {
                'name': 'IEC 60601',
                'type': 'medical_electrical_equipment',
                'description': 'Medical electrical equipment'
            }
        }
    
    def _init_compliance_keywords(self):
        """Initialisiert Compliance-Keywords f√ºr verschiedene Bereiche"""
        self.compliance_keywords = {
            'risk_management': [
                'risiko', 'risk', 'gef√§hrdung', 'hazard', 'schaden', 'harm',
                'wahrscheinlichkeit', 'probability', 'schweregrad', 'severity',
                'risikokontrolle', 'risk control', 'restrisiko', 'residual risk'
            ],
            'quality_assurance': [
                'qualit√§tssicherung', 'quality assurance', 'qualit√§tskontrolle',
                'quality control', 'pr√ºfung', 'testing', 'inspektion', 'inspection',
                '√ºberwachung', 'monitoring', 'bewertung', 'evaluation'
            ],
            'validation': [
                'validierung', 'validation', 'verifikation', 'verification',
                'qualifizierung', 'qualification', 'nachweis', 'evidence',
                'best√§tigung', 'confirmation', 'akzeptanzkriterien', 'acceptance criteria'
            ],
            'documentation': [
                'dokumentation', 'documentation', 'aufzeichnung', 'record',
                'protokoll', 'protocol', 'nachweis', 'evidence',
                'r√ºckverfolgbarkeit', 'traceability', 'archivierung', 'archiving'
            ],
            'regulatory': [
                'regulierung', 'regulation', 'beh√∂rde', 'authority',
                'zulassung', 'approval', 'genehmigung', 'authorization',
                'konformit√§t', 'conformity', 'compliance', 'erf√ºllung'
            ]
        }

    def detect_language(self, text: str) -> Tuple[DocumentLanguage, float, Dict[str, float]]:
        """
        üåç Erkennt die Sprache eines Dokuments mit Konfidenz-Score
        
        Args:
            text: Zu analysierender Text
            
        Returns:
            Tuple[DocumentLanguage, float, Dict[str, float]]: 
            (Erkannte Sprache, Konfidenz, Detailscores)
        """
        if not text or len(text.strip()) < 50:
            return DocumentLanguage.UNKNOWN, 0.0, {}
        
        # Text normalisieren
        text_lower = text.lower()
        words = re.findall(r'\b\w+\b', text_lower)
        
        if len(words) < 10:
            return DocumentLanguage.UNKNOWN, 0.0, {}
        
        language_scores = {}
        
        # Score f√ºr jede Sprache berechnen
        for lang, patterns in self.language_patterns.items():
            score = 0.0
            word_count = 0
            
            # Common words checken
            for word in patterns['common_words']:
                count = words.count(word)
                score += count * 2.0
                word_count += count
            
            # Medical terms checken  
            for term in patterns['medical_terms']:
                if term in text_lower:
                    score += 3.0
                    
            # QMS terms checken
            for term in patterns['qms_terms']:
                if term in text_lower:
                    score += 2.0
            
            # Normalisieren basierend auf Textl√§nge
            if len(words) > 0:
                language_scores[lang.value] = min(score / len(words) * 100, 100.0)
            else:
                language_scores[lang.value] = 0.0
        
        # Beste Sprache ermitteln
        if not language_scores or max(language_scores.values()) < 5.0:
            return DocumentLanguage.UNKNOWN, 0.0, language_scores
        
        best_lang = max(language_scores.items(), key=lambda x: x[1])
        
        # Mixed language detection
        sorted_scores = sorted(language_scores.values(), reverse=True)
        if len(sorted_scores) >= 2 and sorted_scores[1] > sorted_scores[0] * 0.7:
            return DocumentLanguage.MIXED, sorted_scores[0] / 100.0, language_scores
        
        detected_lang = DocumentLanguage(best_lang[0])
        confidence = best_lang[1] / 100.0
        
        self.logger.info(f"üåç Sprache erkannt: {detected_lang.value} (Konfidenz: {confidence:.2%})")
        
        return detected_lang, confidence, language_scores

    def classify_document_type_advanced(self, text: str, filename: str = "") -> Tuple[str, float, List[Tuple[str, float]]]:
        """
        üìä Erweiterte Dokumenttyp-Klassifikation mit 95%+ Genauigkeit
        
        Args:
            text: Dokumenttext
            filename: Dateiname (optional)
            
        Returns:
            Tuple[str, float, List[Tuple[str, float]]]: 
            (Dokumenttyp, Konfidenz, Alternativen)
        """
        if not text:
            return "OTHER", 0.0, []
        
        text_lower = text.lower()
        filename_lower = filename.lower() if filename else ""
        
        type_scores = {}
        
        # Score f√ºr jeden Dokumenttyp berechnen
        for doc_type, patterns in self.document_type_patterns.items():
            score = 0.0
            
            # Keywords im Text suchen
            for keyword in patterns['keywords']:
                keyword_count = text_lower.count(keyword.lower())
                score += keyword_count * 5.0
                
                # Bonus f√ºr Keywords im Dateinamen
                if keyword.lower() in filename_lower:
                    score += 10.0
            
            # Indicators suchen
            for indicator in patterns['indicators']:
                if indicator.lower() in text_lower:
                    score += 3.0
            
            # Strukturelle Hinweise
            for hint in patterns['structure_hints']:
                hint_count = text_lower.count(hint.lower())
                score += hint_count * 2.0
            
            # Normalisierung basierend auf Textl√§nge
            text_length_factor = min(len(text) / 1000, 5.0)
            type_scores[doc_type] = score * text_length_factor
        
        if not type_scores or max(type_scores.values()) < 2.0:
            return "OTHER", 0.3, []
        
        # Scores normalisieren (0-1)
        max_score = max(type_scores.values())
        normalized_scores = {k: v/max_score for k, v in type_scores.items()}
        
        # Sortierte Liste erstellen
        sorted_types = sorted(normalized_scores.items(), key=lambda x: x[1], reverse=True)
        
        best_type = sorted_types[0][0]
        confidence = sorted_types[0][1]
        alternatives = sorted_types[1:4]  # Top 3 Alternativen
        
        self.logger.info(f"üìä Dokumenttyp erkannt: {best_type} (Konfidenz: {confidence:.2%})")
        
        return best_type, confidence, alternatives

    def extract_norm_references(self, text: str) -> List[Dict[str, Union[str, float]]]:
        """
        üìã Extrahiert Norm-Referenzen mit Konfidenz-Scores
        
        Args:
            text: Zu analysierender Text
            
        Returns:
            List[Dict]: Liste erkannter Norm-Referenzen
        """
        norm_references = []
        
        for pattern, norm_info in self.norm_patterns.items():
            matches = re.finditer(pattern, text, re.IGNORECASE)
            
            for match in matches:
                # Kontext um die Referenz extrahieren
                start = max(0, match.start() - 50)
                end = min(len(text), match.end() + 50)
                context = text[start:end].strip()
                
                # Konfidenz basierend auf Kontext bewerten
                confidence = 0.8  # Basis-Konfidenz f√ºr Regex-Match
                
                # Bonus f√ºr relevante Kontextw√∂rter
                context_lower = context.lower()
                if any(word in context_lower for word in ['gem√§√ü', 'according', 'conform', 'compliant']):
                    confidence += 0.1
                if any(word in context_lower for word in ['anforderung', 'requirement', 'standard']):
                    confidence += 0.1
                
                confidence = min(confidence, 1.0)
                
                norm_ref = {
                    'norm_name': norm_info['name'],
                    'norm_type': norm_info['type'],
                    'description': norm_info['description'],
                    'matched_text': match.group(),
                    'context': context,
                    'confidence': confidence,
                    'position': match.start()
                }
                
                norm_references.append(norm_ref)
        
        # Duplikate entfernen (gleiche Norm, verschiedene Positionen)
        unique_norms = {}
        for ref in norm_references:
            norm_key = ref['norm_name']
            if norm_key not in unique_norms or ref['confidence'] > unique_norms[norm_key]['confidence']:
                unique_norms[norm_key] = ref
        
        result = list(unique_norms.values())
        result.sort(key=lambda x: x['confidence'], reverse=True)
        
        self.logger.info(f"üìã {len(result)} Norm-Referenzen gefunden")
        
        return result

    def extract_compliance_keywords(self, text: str) -> List[str]:
        """
        ‚öñÔ∏è Extrahiert Compliance-relevante Keywords
        
        Args:
            text: Zu analysierender Text
            
        Returns:
            List[str]: Liste erkannter Compliance-Keywords
        """
        text_lower = text.lower()
        found_keywords = []
        
        for category, keywords in self.compliance_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    if keyword not in found_keywords:
                        found_keywords.append(keyword)
        
        # Nach H√§ufigkeit sortieren
        keyword_counts = [(kw, text_lower.count(kw.lower())) for kw in found_keywords]
        keyword_counts.sort(key=lambda x: x[1], reverse=True)
        
        result = [kw for kw, count in keyword_counts if count > 0]
        
        self.logger.info(f"‚öñÔ∏è {len(result)} Compliance-Keywords gefunden")
        
        return result

    def calculate_content_similarity(self, text1: str, text2: str) -> float:
        """
        üîç Berechnet inhaltliche √Ñhnlichkeit zwischen zwei Texten
        
        Args:
            text1, text2: Zu vergleichende Texte
            
        Returns:
            float: √Ñhnlichkeits-Score (0.0 - 1.0)
        """
        if not text1 or not text2:
            return 0.0
        
        # Texte normalisieren
        def normalize_text(text):
            # Zu Lowercase, Interpunktion entfernen
            text = text.lower()
            text = re.sub(r'[^\w\s]', ' ', text)
            words = text.split()
            # Stopwords entfernen (einfache Liste)
            stopwords = {'der', 'die', 'das', 'und', 'oder', 'the', 'and', 'or', 'of', 'to', 'in'}
            words = [w for w in words if w not in stopwords and len(w) > 2]
            return set(words)
        
        words1 = normalize_text(text1)
        words2 = normalize_text(text2)
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccard-√Ñhnlichkeit berechnen
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        similarity = intersection / union if union > 0 else 0.0
        
        return similarity

    def assess_content_quality(self, text: str) -> Tuple[float, float]:
        """
        üìà Bewertet Inhaltsqualit√§t und Vollst√§ndigkeit
        
        Args:
            text: Zu bewertender Text
            
        Returns:
            Tuple[float, float]: (Qualit√§ts-Score, Vollst√§ndigkeits-Score)
        """
        if not text:
            return 0.0, 0.0
        
        quality_score = 0.5  # Basis-Score
        completeness_score = 0.3  # Basis-Score
        
        # Textl√§nge bewerten
        text_length = len(text.strip())
        if text_length > 500:
            quality_score += 0.1
        if text_length > 1500:
            quality_score += 0.1
        if text_length > 3000:
            completeness_score += 0.2
        
        # Strukturelle Elemente
        if re.search(r'\d+\.', text):  # Nummerierung
            quality_score += 0.1
            completeness_score += 0.1
        
        if re.search(r'[A-Z][^.!?]*[.!?]', text):  # Vollst√§ndige S√§tze
            quality_score += 0.1
        
        # Fachliche Begriffe
        technical_terms = ['verfahren', 'process', 'anforderung', 'requirement', 
                          'pr√ºfung', 'testing', 'dokumentation', 'documentation']
        term_count = sum(1 for term in technical_terms if term.lower() in text.lower())
        quality_score += min(term_count * 0.05, 0.2)
        
        # Vollst√§ndigkeitsindikatoren
        completeness_indicators = ['zweck', 'purpose', 'anwendungsbereich', 'scope',
                                 'verantwortlichkeit', 'responsibility', 'verfahren', 'procedure']
        completeness_count = sum(1 for indicator in completeness_indicators 
                               if indicator.lower() in text.lower())
        completeness_score += min(completeness_count * 0.1, 0.5)
        
        # Scores begrenzen
        quality_score = min(quality_score, 1.0)
        completeness_score = min(completeness_score, 1.0)
        
        return quality_score, completeness_score

    def comprehensive_analysis(self, text: str, filename: str = "", 
                             existing_documents: Optional[List[Dict]] = None) -> AIAnalysisResult:
        """
        üß† F√ºhrt eine umfassende KI-Analyse durch
        
        Args:
            text: Dokumenttext
            filename: Dateiname
            existing_documents: Liste existierender Dokumente f√ºr Duplikatspr√ºfung
            
        Returns:
            AIAnalysisResult: Umfassende Analyseergebnisse
        """
        self.logger.info(f"üß† Starte umfassende KI-Analyse f√ºr: {filename}")
        
        # 1. Spracherkennung
        language, lang_confidence, lang_details = self.detect_language(text)
        
        # 2. Dokumenttyp-Klassifikation
        doc_type, type_confidence, type_alternatives = self.classify_document_type_advanced(text, filename)
        
        # 3. Norm-Referenzen extrahieren
        norm_refs = self.extract_norm_references(text)
        
        # 4. Compliance-Keywords
        compliance_kw = self.extract_compliance_keywords(text)
        
        # 5. Erweiterte Metadaten
        extracted_keywords = self._extract_keywords(text)
        complexity_score = self._calculate_complexity_score(text)
        risk_level = self._assess_risk_level(text, doc_type)
        
        # 6. Qualit√§tsbewertung
        quality_score, completeness_score = self.assess_content_quality(text)
        
        # 7. Duplikatspr√ºfung
        potential_duplicates = []
        if existing_documents:
            potential_duplicates = self._find_potential_duplicates(text, existing_documents)
        
        result = AIAnalysisResult(
            # Spracherkennung
            detected_language=language,
            language_confidence=lang_confidence,
            language_details=lang_details,
            
            # Dokumentklassifikation
            document_type=doc_type,
            type_confidence=type_confidence,
            type_alternatives=type_alternatives,
            
            # Norm-Referenzen
            norm_references=norm_refs,
            compliance_keywords=compliance_kw,
            
            # Metadaten
            extracted_keywords=extracted_keywords,
            complexity_score=complexity_score,
            risk_level=risk_level,
            
            # Qualit√§tsbewertung
            content_quality_score=quality_score,
            completeness_score=completeness_score,
            
            # √Ñhnlichkeitsanalyse
            potential_duplicates=potential_duplicates
        )
        
        self.logger.info(f"‚úÖ KI-Analyse abgeschlossen: {doc_type} ({type_confidence:.2%} Konfidenz)")
        
        return result

    def _extract_keywords(self, text: str) -> List[str]:
        """Extrahiert relevante Keywords aus dem Text"""
        text_lower = text.lower()
        words = re.findall(r'\b\w{4,}\b', text_lower)  # W√∂rter mit mind. 4 Buchstaben
        
        # H√§ufigkeit z√§hlen
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Top Keywords (min. 2x erw√§hnt)
        keywords = [word for word, freq in word_freq.items() 
                   if freq >= 2 and len(word) > 4]
        keywords.sort(key=lambda x: word_freq[x], reverse=True)
        
        return keywords[:10]  # Top 10

    def _calculate_complexity_score(self, text: str) -> int:
        """Berechnet Komplexit√§ts-Score (1-10)"""
        if not text:
            return 1
        
        score = 1
        
        # Textl√§nge
        if len(text) > 1000:
            score += 1
        if len(text) > 3000:
            score += 1
        if len(text) > 5000:
            score += 1
        
        # Technische Begriffe
        technical_indicators = ['verfahren', 'prozess', 'system', 'anforderung', 
                               'spezifikation', 'validierung', 'kalibrierung']
        tech_count = sum(1 for term in technical_indicators if term in text.lower())
        score += min(tech_count, 3)
        
        # Norm-Referenzen
        norm_count = len(re.findall(r'ISO\s*\d+|IEC\s*\d+|EN\s*\d+', text, re.IGNORECASE))
        score += min(norm_count, 2)
        
        return min(score, 10)

    def _assess_risk_level(self, text: str, doc_type: str) -> str:
        """Bewertet Risiko-Level basierend auf Inhalt und Typ"""
        risk_indicators = text.lower().count('risiko') + text.lower().count('risk')
        safety_indicators = text.lower().count('sicherheit') + text.lower().count('safety')
        
        # Dokumenttyp-basierte Risikobewertung
        high_risk_types = ['RISK_ASSESSMENT', 'VALIDATION_PROTOCOL', 'AUDIT_REPORT']
        medium_risk_types = ['SOP', 'WORK_INSTRUCTION', 'CALIBRATION_PROCEDURE']
        
        if doc_type in high_risk_types or risk_indicators > 5 or safety_indicators > 3:
            return "HOCH"
        elif doc_type in medium_risk_types or risk_indicators > 2:
            return "MITTEL"
        else:
            return "NIEDRIG"

    def _find_potential_duplicates(self, text: str, existing_docs: List[Dict]) -> List[Dict]:
        """Findet potentielle Duplikate basierend auf √Ñhnlichkeit"""
        duplicates = []
        
        for doc in existing_docs:
            if 'extracted_text' in doc and doc['extracted_text']:
                similarity = self.calculate_content_similarity(text, doc['extracted_text'])
                
                if similarity > 0.7:  # 70% √Ñhnlichkeit
                    duplicates.append({
                        'document_id': doc.get('id'),
                        'title': doc.get('title', 'Unbekannt'),
                        'similarity_score': similarity,
                        'reason': f"Hohe Text√§hnlichkeit ({similarity:.1%})"
                    })
        
        duplicates.sort(key=lambda x: x['similarity_score'], reverse=True)
        return duplicates[:3]  # Top 3 √§hnlichste Dokumente


# Global AI Engine Instance
ai_engine = AdvancedAIEngine() 